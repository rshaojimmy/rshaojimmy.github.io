<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<META name=keywords 
content="Rui Shao, Shao Rui, rui shao, shao rui, ruishao, shaorui, 邵睿, hkbu, HKBU, Hong Kong Baptist University">
<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="./jemdoc.css" type="text/css">
    <title>Rui Shao's Homepage</title>
</head>



<body data-feedly-mini="yes">

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670" >
                <div id="toptitle">                 
                    <h1>Rui Shao (邵睿) &nbsp; </h1><h1> <!-- <img src="./zzz_files/zizhao.jpg" width="190" style="margin-bottom:-10px"> -->
                </h1></div>
                <p>
                    <h3>Professor
                    </h3></div> 
                    <br>
                    <a href="http://cs.hitsz.edu.cn/">School of Computer Science and Technology</a>, 
                    <br>
                    <br>
                    <a href="https://www.hitsz.edu.cn/index.html">Harbin Institute of Technology (Shenzhen)</a>
                    <br>
                    <br>
                    Email: shaorui[AT]hit.edu.cn, rshaojimmy[AT]gmail.com
                    <br>
                    <br>
                    [<a href="https://scholar.google.com/citations?hl=en&user=9Vc--XsAAAAJ/&user=9Vc--XsAAAAJ">Google Scholar</a>]
                    [<a href="https://github.com/rshaojimmy">GitHub</a>]
                    [<a href="https://www.linkedin.com/in/rui-shao-0623b6122/">LinkedIn</a>]  
                    [<a href="linkfile/CV/RuiSHAO_CV_English_2024.12.02.pdf">CV</a>]                    
                    [<a href="linkfile/CV/RuiSHAO_CV_Chinese_2024.12.02.pdf">中文简历</a>]
                    [<a href="http://faculty.hitsz.edu.cn/shaorui">中文主页</a>]                    
                </p>
            </td>
            <td rowspan="1">
                <img src="./linkfile/portrait/yinchuan.jpg" border="0" width="400" align="bottom" ><br>
            </td>
        </tr>
                <!-- <br> -->
                <!-- <colgroup><col height="50" width="270">
                </colgroup> -->
                <!-- <tbody> -->
                <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 250px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>02/2025: Three paper about ego-centric video MLLM, MLLM agent and embodied MLLM are accepted by <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.                                                
                        <li>01/2025: One paper about SmartPhone Multimodal Agent accepted by <a href="https://iclr.cc//">ICLR 2025</a>.                        
                        <li>12/2024: The extension of our ECCV 2022 paper (SeqDeepFake) has been accepted by <a href="https://www.springer.com/journal/11263/"> International Journal of Computer Vision (IJCV). Cheers! </a>
                        <li>10/2024: We have built GitHub Orgnization of <a href="https://github.com/JiuTian-VL">JiuTian-VL</a> that will post all information about our JiuTian MLLM.
                        <li>10/2024: I have one paper about the adapter of large vision models accepted by <a href="https://www.springer.com/journal/11263/"> International Journal of Computer Vision (IJCV)</a>. Cheers!
                        <li>10/2024: Two papers about MLLMs are accepted at <a href="https://neurips.cc/">NeurIPS 2024</a>, including contributions from a 2nd-year undergraduate. Cheers!
                        <li>07/2024: One paper about Audio-Visual Multimodal Large Language Model accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
                        <li>02/2024: Our Multimodal Large Language Model (MLLM)-<a href="https://arxiv.org/pdf/2311.11860.pdf"> JiuTian-LION </a> has been accepted by <a href="https://cvpr.thecvf.com/"> CVPR 2024</a>. Cheers!
                        <li>02/2024: The extension of our <a href="https://github.com/rshaojimmy/MultiModal-DeepFake">CVPR 2023 paper</a> has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</a>. Cheers! 
                        <li>11/2023: We have released the arXiv paper for our Multimodal Large Language Model (MLLM)-<a href="https://arxiv.org/pdf/2311.11860.pdf"> JiuTian-LION </a>. Enjoy it!
                        <li>08/2023: We have built the GitHub Repo for our Multimodal Large Language Model (MLLM)-<a href="https://github.com/rshaojimmy/JiuTian"> JiuTian </a>. Enjoy it!
                        <li>04/2023: I have released the code and dataset of our CVPR 2023 work in our <a href="https://github.com/rshaojimmy/MultiModal-DeepFake"> GitHub Repo </a>. Enjoy it!
                        <li>02/2023: I have one paper accepted by <a href="https://cvpr2023.thecvf.com/"> CVPR 2023</a>. Code and dataset will be released soon. Please stay tuned! </a>
                        <li>07/2022: I have one paper accepted by <a href="https://eccv2022.ecva.net/"> ECCV 2022</a>. We have released the code and dataset in our <a href="https://rshaojimmy.github.io/Projects/SeqDeepFake"> project page </a>
                        <li>05/2022: I have released the code of Federated Generalized Face Presentation Attack Detection in TNNLS 2022. <a href="https://github.com/rshaojimmy/TNNLS2022-FedGPAD"> Codes </a>
                        <li>04/2022: I have one paper accepted by <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems"> IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </a>.
                        <li>03/2022: I have released the code of Open-set Adversarial Defense with Clean-Adversarial Mutual Learning in IJCV 2022. <a href="https://github.com/rshaojimmy/IJCV2022-OSDN-CAML"> Codes </a>
                        <li>01/2022: The extension of our ECCV 2020 paper has been accepted by <a href="https://www.springer.com/journal/11263/"> International Journal of Computer Vision (IJCV). </a>
                        <li>08/2020: I have released the code of Open-set Adversarial Defense in ECCV 2020. <a href="https://github.com/rshaojimmy/ECCV2020-OSAD"> Codes </a>
                    	<li>07/2020: I have one paper accepted by <a href="https://eccv2020.eu/"> ECCV 2020</a>. See you online!  
                        <li>11/2019: I have released the code of Regularized Fine-grained Meta Face Anti-spoofing in AAAI 2020. <a href="https://github.com/rshaojimmy/AAAI2020-RFMetaFAS"> Codes </a>
                        <li>11/2019: I have one paper accepted by <a href="https://aaai.org/Conferences/AAAI-20/#"> AAAI 2020</a>. See you at New York City, USA!                         
                        <li>10/2019: I have one paper one paper accepted by <a href="https://digital-library.theiet.org/content/journals/iet-ipr"> IET Image Processing. </a>
                        <li>07/2019: I have released the code of Multi-adversarial Discriminative Deep Domain Generalization for FAS in CVPR 2019. <a href="https://github.com/rshaojimmy/CVPR2019-MADDoG"> Codes </a>                           
                        <li>07/2019: I have released the code of Joint Discriminative Learning of Deep Dynamic Textures for 3D Mask FAS in TIFS 2019. <a href="https://github.com/rshaojimmy/TIFS2019"> Codes </a>      
                        <li>02/2019: I have one paper accepted by <a href="http://cvpr2019.thecvf.com/"> CVPR 2019</a>. See you at Long Beach, USA!                          
                        <li>02/2019: One paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=41"> IEEE Transactions on Industrial Electronics. </a> 
                        <li>08/2018: I have one paper accepted by <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security"> IEEE Transactions on Information Forensics and Security </a>.
                        <li>07/2018: I have released the code of Hierarchical Adversarial Deep Domain Adaptation in ACMMM 2018. <a href="https://github.com/rshaojimmy/ACMMM2018-HADDA"> Codes </a>
                        <li>07/2018: I have one paper accepted by <a href="http://www.acmmm.org/2018/"> ACM MM 2018</a>. See you at Seoul, Korea!                            
                        <li>08/2018: I have a new homepage.
                    </ul>
                    </div>
                </td>
            

        </tbody>
</table>

<h2>About Me</h2>
<p>
    I am currently a Professor at <a href="http://cs.hitsz.edu.cn/">School of Computer Science and Technology</a>, <a href="https://www.hitsz.edu.cn/index.html">Harbin Institute of Technology (Shenzhen)</a>. Prior to that, I was a postdoc at Nanyang Technological University, Singapore, working with <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a> and <a href="https://www.mmlab-ntu.com/person/ccloy/">Prof. Chen Change Loy</a>. 
</p> 
<p>
    I received my PhD degree from <a href="http://www.comp.hkbu.edu.hk/v1/"> Department of Computer Science</a>, Hong Kong Baptist University in 2021, supervised by <a href="http://www.comp.hkbu.edu.hk/~pcyuen/"> Prof. Pong C. Yuen</a>, and my bachelor degree from <a href="http://www.sice.uestc.edu.cn/"> School of Information and Communication Engineering</a>, <a href="http://www.uestc.edu.cn/"> University of Electronic Science and Technology of China (UESTC) </a> in 2015. I also spent a memorable high-school time in <a href="http://www.sfls.net.cn/"> Shenzhen Foreign Languages School.</a> I visited the Johns Hopkins University for 6 months in 2020.
</p> 
<p>
    I am interested in computer vision and multimodal learning. My current research focuses on Multimodal Large Language Model (MLLM) (e.g., <a href="https://github.com/rshaojimmy/JiuTian"> "JiuTian" MLLM </a>) and its agent applications (e.g., Embodied Agent and GUI Agent).
    <p>
        <font color="red"><i>Looking for self-motivated Ph.D/M.S./Undergraduate students. </i>[2024硕士/博士招生, 4-5名硕士, 2名博士]</font>
        <br>
        <font color="red"><i>Looking for PostDocs in computer vision and MLLM.</i></font>
    </p>
    
    <!-- I am interested in Robustness (e.g., Fake News Detection, DeepFake Detection, Face Anti-spoofing, Adversarial Attacks/Defense) and Generalization (e.g., Domain Adaptation/Generalization, Meta Learning, and Federated Learning) of Computer Vision and Machine Learning.</p><br>  -->
<!--     My current research focuses on exploiting the adaptive and generalized deep learning to develop a secure face recognition system with good generalization ability. -->

<!--     <p style='color:red'><strong> I am looking for a postdoctoral position or a full-time job. Please feel free to drop me an email.</strong></p> -->
</p> 
    

<h2>Biography</h2>

<li> 2021-2023, Research Fellow, <a href="https://www.mmlab-ntu.com/index.html"> MMLab@NTU</a>, Singapore 

<li> 2021.7- 2021.11, Researcher, <a href="https://www.sensetime.com/en"> SenseTime</a>, Shenzhen, China, participating project of </a> <a href="https://github.com/open-mmlab/mmhuman3d">MMhuman3D</a> codebase.

<li> 2017-2021, Ph.D., <a href="http://www.comp.hkbu.edu.hk/v1/"> Department of Computer Science, </a> <a href="https://www.hkbu.edu.hk/eng/main/index.jsp">Hong Kong Baptist University</a>, Hong Kong, China 

<li> 2020.2-2020.7, Visiting scholar at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, working with <a href="https://engineering.jhu.edu/vpatel36/vishal-patel/"> Prof. Vishal M Patel</a>, Baltimore, U.S. 

<!-- <li> 2015-2016, M.S., <a href="http://cs.whu.edu.cn/aspx/enmain/">School of Computer Science</a>, <a href="https://en.whu.edu.cn/"> Wuhan University, </a> Wuhan, China  -->

<li> 2011-2015, B.S., <a href="http://www.sice.uestc.edu.cn/"> School of Information and Communication Engineering</a>, <a href="http://www.uestc.edu.cn/"> University of Electronic Science and Technology of China</a>, Chengdu, China  


<h2>Pre-prints (* denotes corresponding authors) </a>  </h2> 
    <table id="Pre-prints" width="100%">
    <tbody>

        <tr>    
            <td width="300">
                <img src="./linkfile/CVPR2025/falcon.jpg" width="260px" height="140px">
                </td>
                <td>
            <p> FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers  </a></p>
            Renshan Zhang, <b>Rui Shao</b>*, Gongwei Chen, Kaiwen Zhou, Weili Guan, Liqiang Nie*</p>
            [<a href="https://arxiv.org/pdf/2501.16297?">arXiv</a>]
        </tr> 
        <tr><td>   <br>  </td></tr>    

    <tr>    
        <td width="300">
            <img src="./linkfile/ACL2024sub/ECoT.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought  </a></p>
        Zaijing Li, <b>Rui Shao</b>*, Gongwei Chen, Dongmei Jiang, Liqiang Nie</p>
        [<a href="https://arxiv.org/pdf/2401.06836.pdf">arXiv</a>]
        <!-- [<a href="">Code</a>] -->
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/NeurIPS2024/tokencompressor.jpg" width="260px" height="130px">
            </td>
            <td>
        <p> Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding  </a></p>
        Renshan Zhang, Yibo Lyu, <b>Rui Shao</b>*, Gongwei Chen, Weili Guan, Liqiang Nie*</p>
        [<a href="https://arxiv.org/pdf/2407.14439">arXiv</a>]
        <!-- [<a href="">Code</a>] -->
    </tr> 
    <tr><td>   <br>  </td></tr>    

</tbody></table>

<h2>Selected Publications (* denotes corresponding authors) </a>  </h2> 
<table id="tbPublications" width="100%">

    <tr>    
        <td width="300">
            <img src="./linkfile/CVPR2025/LION-FS.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant </a></p>
        Wei Li, Bing Hu, <b>Rui Shao</b>*, Leyang Shen, Liqiang Nie</p>
        <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. </i></p>
        [<a href="">arXiv</a>]
        [<a href="">Code</a>]
        [<a href="">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
    
    <tr>    
        <td width="300">
            <img src="./linkfile/CVPR2025/optimus-2.jpg" width="260px" height="140px">
            </td>
            <td>
        <p> Optimus-2: Mulitimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy </a></p>
        Zaijing Li, Yuquan Xie, <b>Rui Shao</b>*, Gongwei Chen, Dongmei Jiang, Liqiang Nie*</p>
        <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. </i></p>
        [<a href="">arXiv</a>]
        [<a href="">Code</a>]
        [<a href="">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>    

    <tr>    
        <td width="300">
            <img src="./linkfile/CVPR2025/KStar.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> Spatial-Temporal Graph Diffusion Policy with Kinematics Modeling for Bimanual Robotic Manipulation </a></p>
        Qi Lv, Hao Li, Xiang Deng, <b>Rui Shao</b>, Yinchuan Li, Jianye HAO, Longxiang Gao, MICHAEL YU WANG, Liqiang Nie
        <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. </i></p>
        [<a href="">arXiv</a>]
        [<a href="">Code</a>]
        [<a href="">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>   

    <tr>    
        <td width="300">
            <img src="./linkfile/ICLR2025/spa-bench.jpg" width="260px" height="130px">
            </td>
            <td>
        <p> SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation </a></p>
        Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Kaiwen Zhou, <b>Rui Shao</b>*, Liqiang Nie, Yasheng Wang, Jianye Hao, Jun Wang, Kun Shao*</p>
        <p><i>The Thirteenth International Conference on Learning Representations (<b>ICLR</b>), 2025. </i></p>
        [<a href="https://arxiv.org/pdf/2410.15164">arXiv</a>]
        [<a href="https://github.com/ai-agents-2030/SPA-Bench">Code</a>]
        [<a href="https://ai-agents-2030.github.io/SPA-Bench//">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/NeurIPS2024/Optimus-1.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks </a></p>
        Zaijing Li, Yuquan Xie, <b>Rui Shao</b>*, Gongwei Chen, Dongmei Jiang, Liqiang Nie*</p>
        <p><i>Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2408.03615">arXiv</a>]
        [<a href="https://github.com/JiuTian-VL/Optimus-1">Code</a>]
        [<a href="https://cybertronagent.github.io/Optimus-1.github.io/">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/NeurIPS2024/MoME.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models </a></p>
        Leyang Shen, Gongwei Chen, <b>Rui Shao</b>*, Weili Guan, Liqiang Nie*</p>
        <p><i>Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2407.12709">arXiv</a>]
        [<a href="https://github.com/JiuTian-VL/MoME">Code</a>]
        <!-- [<a href="https://cybertronagent.github.io/Optimus-1.github.io/">Project Page</a>] -->
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/CVPR2024/LION.jpg" width="260px" height="90px">
            </td>
            <td>
        <p> LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge </a></p>
        Gongwei Chen, Leyang Shen, <b>Rui Shao</b>*, Xiang Deng, Liqiang Nie*</p>
        <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2311.11860.pdf">arXiv</a>]
        [<a href="https://github.com/JiuTian-VL/JiuTian-LION">Code</a>]
        [<a href="https://rshaojimmy.github.io/Projects/JiuTian-LION">Project Page</a>]
        [<a href="https://mp.weixin.qq.com/s/lIhXb4YM7vjh8VidmnA_0w">Press</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/ECCV2022/ECCV2022-ext.jpg" width="260px" height="120px">
            </td>
            <td>
        <p> Robust Sequential DeepFake Detection  </a></p>
        <b>Rui Shao</b>, Tianxing Wu, Ziwei Liu</p>
        <p><i>International Journal of Computer Vision (<b>IJCV</b>), 2025</i></p>
        [<a href="https://arxiv.org/pdf/2309.14991.pdf">arXiv</a>]
        [<a href="./linkfile/IJCV2025-SeqDeepFake.pdf">PDF</a>]
        [<a href="https://github.com/rshaojimmy/SeqDeepFake">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/DeepFake-Adatper/deepfake-adapter.jpg" width="260px" height="160px">
            </td>
            <td>
        <p> DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection  </a></p>
        <b>Rui Shao</b>, Tianxing Wu, Liqiang Nie, Ziwei Liu</p>
        <p><i>International Journal of Computer Vision (<b>IJCV</b>), 2025</i></p>
        [<a href="https://arxiv.org/pdf/2306.00863.pdf">arXiv</a>]
        [<a href="./linkfile/IJCV2025-DeepFake-Adapter.pdf">PDF</a>]
        [<a href="https://github.com/rshaojimmy/DeepFake-Adapter">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tbody>
        <tr>  
        <td width="300">
            <img src="./linkfile/CVPR2023/CVPR2023-ext.jpg" width="260px" height="90px">
            </td>
            <td>
        <p> Detecting and Grounding Multi-Modal Media Manipulation and Beyond  </a></p>
        <b>Rui Shao</b>, Tianxing Wu, Jianlong Wu, Liqiang Nie, Ziwei Liu</p>
        <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024</i></p>
        [<a href="https://arxiv.org/pdf/2309.14203.pdf">arXiv</a>]
        [<a href="https://github.com/rshaojimmy/MultiModal-DeepFake">Code</a>]
        [<a href="https://rshaojimmy.github.io/Projects/MultiModal-DeepFake">Project Page</a>]
        [<a href="https://mp.weixin.qq.com/s/gHcRSKDNaWq9b7b5-GA9PA">Press</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tbody>
        <tr>  
        <td width="300">
            <img src="./linkfile/ECCV2024/ECCV2024.jpg" width="270px" height="100px">
            </td>
            <td>
        <p> CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios </a></p>
        Qilang Ye, Zitong Yu, <b>Rui Shao</b>, Xinyu Xie, Philip Torr, Xiaochun Cao</p>
        <p><i>European Conference on Computer Vision (<b>ECCV</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2403.04640">arXiv</a>]
        [<a href="https://github.com/rikeilong/Bay-CAT">Code</a>]
        [<a href="https://github.com/rikeilong/Bay-CAT?tab=readme-ov-file">Project Page</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tbody>
        <tr>
            <td width="300">
            <img src="./linkfile/CVPR2023/CVPR2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Detecting and Grounding Multi-Modal Media Manipulation  </a></p>
            <b>Rui Shao</b>, Tianxing Wu, Ziwei Liu</p>
            <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2304.02556">arXiv</a>]
            [<a href="https://github.com/rshaojimmy/MultiModal-DeepFake">Code</a>]
            [<a href="https://rshaojimmy.github.io/Projects/MultiModal-DeepFake">Project Page</a>]
            [<a href="https://mp.weixin.qq.com/s/gHcRSKDNaWq9b7b5-GA9PA">Press</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tbody>
        <tr>
            <td width="300">
            <img src="./linkfile/bmvc2023.jpg" width="260px" height="120px">
            </td>
            <td>
            <p> Video Infilling with Rich Motion Prior  </a></p>
            Xinyu Hou, Liming Jiang, <b>Rui Shao</b>, Chen Change Loy</p>
            <p><i>British Machine Vision Conference (<b>BMVC</b>), 2023. </i></p>
            [<a href="">arXiv</a>]
            [<a href="">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

      <tr>
            <td width="300">
            <img src="./linkfile/ECCV2022/ECCV2022.png" width="260px" height="120px">
            </td>
            <td>
            <p> Detecting and Recovering Sequential DeepFake Manipulation  </a></p>
            <b>Rui Shao</b>, Tianxing Wu, Ziwei Liu</p>
            <p><i>European Conference on Computer Vision (<b>ECCV</b>), 2022. </i></p>
            [<a href="https://arxiv.org/pdf/2207.02204.pdf">arXiv</a>]
            [<a href="https://github.com/rshaojimmy/SeqDeepFake">Code</a>]
            [<a href="https://rshaojimmy.github.io/Projects/SeqDeepFake">Project Page</a>]
            [<a href="./linkfile/ECCV2022/poster_1347.pdf">Poster</a>]
            <!-- [<a href="https://lifehkbueduhk-my.sharepoint.com/:v:/g/personal/16483782_life_hkbu_edu_hk/EUcK_Yzjw11Gm7dGIlbQLi8BoB_WT4pp2NH-uuZGwGPHUg?e=llCf9V">Video</a>] -->
            [<a href="https://www.marktechpost.com/2022/07/17/researchers-at-ntu-singapore-propose-seq-deepfake-transformer-seqfakeformer-for-detecting-and-recovering-sequential-deepfake-manipulations/">Press1</a>]
            [<a href="https://mp.weixin.qq.com/s/KJ7b6pmIp6RRrQ_g9GGJUw">Press2</a>]
            [<a href="https://mp.weixin.qq.com/s/4IdI87iwsFS7uuTc1_8u2g">Press3</a>]
            [<a href="https://mp.weixin.qq.com/s/wJ-8lf-94S38FJrWpIFXQA">Press4</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

        <tr>
            <td width="300">
            <img src="./linkfile/ECCV2020/IJCV2022.jpg" width="260px" height="130px">
            </td>
            <td>
            <p> Open-set Adversarial Defense with Clean-Adversarial Mutual Learning  </a></p>
            <b>Rui Shao</b>, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel</p>
      <p><i>International Journal of Computer Vision (<b>IJCV</b>), 2022</i></p>
        [<a href="https://arxiv.org/pdf/2202.05953.pdf">arXiv</a>]
        [<a href="https://link.springer.com/content/pdf/10.1007/s11263-022-01581-0.pdf">PDF</a>]
        [<a href="https://github.com/rshaojimmy/IJCV2022-OSDN-CAML">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

      <tr>
            <td width="300">
            <img src="./linkfile/IJCB2020/fedpadext.jpg" width="260px" height="160px">
            </td>
            <td>
            <p> Federated Generalized Face Presentation Attack Detection  </a></p>
            <b>Rui Shao</b>, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel</p>
            <p><i>IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), 2022. </i></p>
            [<a href="https://arxiv.org/pdf/2104.06595.pdf">arXiv</a>]
            [<a href="https://ieeexplore.ieee.org/document/9780603">PDF</a>]
            [<a href="https://github.com/rshaojimmy/TNNLS2022-FedGPAD">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>


    <tr>
            <td width="300">
            <img src="./linkfile/FG2021.jpg" width="250px" height="140px">
            </td>
            <td>
            <p> Federated Test-Time Adaptive Face Presentation Attack Detection with Dual-Phase Privacy Preservation </a></p>
            <b>Rui Shao</b>, Bochao Zhang, Pong C. Yuen, Vishal M. Patel</p>
            <p><i>IEEE International Conference on Automatic Face and Gesture Recognition (<b>FG</b>), 2021</i></p>
            [<a href="https://arxiv.org/pdf/2110.12613.pdf">arXiv</a>]
            [<a href="https://ieeexplore.ieee.org/document/9666952">PDF</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./linkfile/MICCAI2021.jpg" width="250px" height="140px">
            </td>
            <td>
            <p> Focusing on Clinically Interpretable Features: Selective Attention Regularization for Liver Biopsy Image Classification </a></p>         
            Chong Yin, Siqi Liu, <b>Rui Shao</b>, Pong C. Yuen,</p>
            <p><i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2021</i></p>
            [<a href="https://link.springer.com/content/pdf/10.1007/978-3-030-87240-3_15.pdf">PDF</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>



    <tr>
            <td width="300">
            <img src="./linkfile/ECCV2020/eccv2020.png" width="250px" height="140px">
            </td>
            <td>
            <p> Open-set Adversarial Defense </a></p>
            <b>Rui Shao</b>, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel</p>
            <p><i>European Conference on Computer Vision (<b>ECCV</b>), 2020</i></p>
            [<a href="https://arxiv.org/pdf/2009.00814.pdf">arXiv</a>]
            [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620664.pdf">PDF</a>]
            [<a href="https://github.com/rshaojimmy/ECCV2020-OSAD">Code</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>



    <tr>
            <td width="300">
            <img src="./linkfile/AAAI2020/aaai2020.png" width="250px" height="140px">
            </td>
            <td>
            <p> Regularized Fine-grained Meta Face Anti-spooﬁng </a></p>
            <b>Rui Shao</b>, Xiangyuan Lan, Pong C. Yuen</p>
            <p><i>Thirty-Fourth AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020</i></p>
            [<a href="https://arxiv.org/pdf/1911.10771.pdf">arXiv</a>]
            [<a href="./linkfile/AAAI2020/6873-Article Text-10102-1-10-20200525.pdf">PDF</a>]
            [<a href="./linkfile/AAAI2020/AAAI2020_RFMetaFAS.pdf"/>Poster</a>]
            [<a href="https://github.com/rshaojimmy/AAAI2020-RFMetaFAS">Code</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./linkfile/CVPR2019/cvpr2019.jpg" width="240px" height="180px">
            </td>
            <td>
            <p> Multi-adversarial Discriminative Deep Domain Generalization for Face Presentation Attack Detection </a></p>
            <b>Rui Shao</b>, Xiangyuan Lan, Jiawei Li, Pong C. Yuen</p>
            <p><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019</i></p>
            [<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Shao_Multi-Adversarial_Discriminative_Deep_Domain_Generalization_for_Face_Presentation_Attack_Detection_CVPR_2019_paper.html">PDF</a>]
            [<a href="./linkfile/CVPR2019/RuiShao_CVPR_poster_v3.pdf"/>Poster</a>]
            [<a href="https://github.com/rshaojimmy/CVPR2019-MADDoG">Code</a>]
            [<a href="https://drive.google.com/open?id=1Nsi4dxb17ZiuegMVKLPo8cr46EMjhEH8">Model</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./linkfile/tifs2018.png" width="270px" height="130px">
            </td>
            <td>
            <p> Joint Discriminative Learning of Deep Dynamic Textures for 3D Mask Face Anti-spoofing </a></p>
            <b>Rui Shao</b>, Xiangyuan Lan, Pong C. Yuen</p>
            <p><i> IEEE Transactions on Information Forensics and Security (<b>TIFS</b>), 2019</i></p>
            [<a href="https://ieeexplore.ieee.org/document/8453011">PDF</a>]
            [<a href="https://github.com/rshaojimmy/TIFS2019">Code</a>]    
    </tr>
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./linkfile/iet2019.jpg" width="270px" height="130px">
            </td>
            <td>
            <p> Adversarial Auto-encoder for Unsupervised Deep Domain Adaptation </a></p>
            <b>Rui Shao</b>, Xiangyuan Lan</p>
            <p><i> IET Image Processing. (<b>IET-IPR</b>), 2019</i></p>
            [<a href="https://digital-library.theiet.org/content/journals/10.1049/iet-ipr.2018.6687">PDF</a>]    
    </tr>
    <tr><td>   <br>  </td></tr>                    

    <tr>
            <td width="300">
            <img src="./linkfile/ACMMM2018/acmmm2018.png" width="270px" height="120px" > 
            </td>
            <td>
            <p> Feature Constrained by Pixel: Hierarchical Adversarial Deep Domain Adaptation</a></p>
            <b>Rui Shao</b>, Xiangyuan Lan, Pong C. Yuen</p>
            <p><i> ACM international conference on Multimedia (<b>ACM MM</b>), 2018</i></p>
            [<a href="https://dl.acm.org/citation.cfm?id=3240562">PDF</a>]
            [<a href="./linkfile/ACMMM2018/acmmm_poster.pdf"/>Poster</a>]
            [<a href="https://github.com/rshaojimmy/ACMMM2018-HADDA">Code</a>]


            </p></td>
    <tr><td>   <br>  </td></tr>



    <tr>
            <td width="300">
            <img src="./linkfile/ijcb2017.png" width="260px" height="110px" > 
            </td>
            <td>
            <p> Deep Convolutional Dynamic Texture Learning with Adaptive Channel-discriminability for 3D Mask Face Anti-spoofing </p>
            <b>Rui Shao</b>, Xiangyuan Lan, Pong C. Yuen</p>
            <p><i> International Joint Conference on Biometrics (<b>IJCB</b>), 2017</i></p>
            [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8272765">PDF</a>] 
                
            </p></td>
    <tr><td>   <br>  </td></tr>


    <tr>
        <td width="300">
        <img src="./linkfile/tie2019.jpg" width="260px" height="110px">
        </td>
        <td>
        <p> Learning Modality-Consistency Feature Templates: A Robust RGB-Infrared Tracking System</p>
        Xiangyuan Lan, Mang Ye, <b>Rui Shao</b>, Bineng Zhong, Pong C. Yuen, Huiyu Zhou
        <p><i> IEEE Transactions on Industrial Electronics (<b>TIE</b>), 2019</i></p>
        [<a href="https://ieeexplore.ieee.org/document/8643077">PDF</a>]       
</tr> 
<tr><td>   <br>  </td></tr>

    <!-- <tr>
            <td width="300">
            <img src="./linkfile/tracvid2016.jpg" width="200px" > 
            </td>
            <td>
            <p> <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv15.papers/nercms.pdf"> WHU-NERCMS at TREVCID 2015:Instance search task </a></p>
            Lei Yao, Mang Ye, Dongjing Liu, <b>Rui Shao</b>, Tao Liu, Jun Liu, Zheng Wang, Chao Liang.</p>
            <p><i> Participant Notebook Paper. (<b>TRECVID</b>), 2015</i>

            </p></td>
    <tr><td>   <br>  </td></tr> -->

 
</tbody></table>


<!-- <h2>Awards</h2>
<table id="Awards" border="0" width="100%">

    <tbody>
    <li><a href="http://www.comp.hkbu.edu.hk/v1/?pid=48"> 2018/19 Computer Science Department RPg Performance Award</a> 
    <li><a href="http://www.comp.hkbu.edu.hk/v1/?pid=48"> 2017/18 Computer Science Department RPg Performance Award</a>

<tr><td>   <br>  </td></tr> -->

</tbody></table>

<h2>Services</h2>
<table id="Services" border="0" width="100%">

    <tbody>

Area Chair:
<li>ACM MM 2024
<li>BMVC 2024

<!-- Invited Reviewer for: 
    <li>TPAMI, IJCV, TIP, TNNLS, TIFS, TCSVT, JSTSP, PR
    <li>CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, IJCAI, ACM MM
    <li>Program Committee Member for: AAAI 2021 2022 2023 -->

<tr><td>   <br>  </td></tr>

</tbody></table>


<h2>Collaborators</h2>
<table id="tbTeaching" border="0" width="100%" >
    <tbody>
    <li><a href="https://liuziwei7.github.io/"> Prof. Ziwei Liu</a>, Nanyang Technological University

    <li><a href="https://engineering.jhu.edu/vpatel36/vishal-patel/"> Prof. Vishal M Patel</a>, Johns Hopkins University

    <li><a href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl=en"> Dr. Xiangyuan Lan</a>, Peng Cheng Laboratory

    <li><a href="https://pages.jh.edu/~pperera3/"> Dr. Pramuditha Perera</a>, Johns Hopkins University, AWS AI Lab
        
    </tbody>
</table>

<h2></h2>

<!-- <a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?rshaojimmy"
border="0" alt="Web Site Hit Counter"></a>
<br><a href="https://www.easycounter.com/">Unique visitors since Apr 2019</a>

<!-- <script type="text/javascript" src="//ra.revolvermaps.com/0/0/8.js?i=0t4x80je5xv&amp;m=0&amp;c=baff00&amp;cr1=baff00&amp;f=arial&amp;l=33" async="async"></script> -->

<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=yQtd-EynvtzIDuSGynEuIlVZWeRz8dMLMy3lDGuyPAc'></script> --> 

<!-- <div id="footer">
    <div id="footer-text"></div> -->

</div>
</body>
</html>